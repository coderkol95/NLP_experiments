{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Init","metadata":{"execution":{"iopub.status.busy":"2022-09-09T10:35:35.795723Z","iopub.execute_input":"2022-09-09T10:35:35.796222Z","iopub.status.idle":"2022-09-09T10:35:35.802324Z","shell.execute_reply.started":"2022-09-09T10:35:35.796185Z","shell.execute_reply":"2022-09-09T10:35:35.801004Z"}}},{"cell_type":"code","source":"!pip install pytorch-lightning\nimport pytorch_lightning as pl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nimport re\nfrom nltk.tokenize import RegexpTokenizer\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom sklearn.preprocessing import OneHotEncoder\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"topics = glob.glob('../input/poemsdataset/topics/*')\ntopics = [re.sub('../input/poemsdataset/topics/','',x) for x in topics]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poems = pd.DataFrame(columns=['poem','topic'])\n\nfor topic in topics:\n    path = glob.glob(f'../input/poemsdataset/topics/{topics[0]}/*')\n    \n    for p in path:\n        with open(p,'r') as file:\n            poem = file.read()\n        poems.loc[len(poems.index)] = [poem, topic]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poems = poems.sample(frac=1)\n# poems.to_csv('poems.csv')\n# poems = pd.read_csv('poems.csv')\n# poems.drop(['Unnamed: 0'],axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = RegexpTokenizer(pattern='\\w+')\n\ndef tokenizer_func(x):\n    return tokenizer.tokenize(x.lower())\n\npoems['poem'] = poems['poem'].apply(tokenizer_func)\npoems['poem'] = poems['poem'].apply(lambda x : [y.lower() for y in x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.percentile(np.array([len(x) for x in poems.poem.values]), 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_sequences(text):\n    \n    if len(text) <= 105:\n        text.extend(['']*(105 - len(text)))\n        \n    else:\n        text = text[:105]\n        \n    return text\n\npoems['poem'] = poems['poem'].apply(pad_sequences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# topics = poems['topic'].unique().tolist()\n# topic_dict = dict(zip(topics, np.arange(len(topics))))\n# poems['topic'] = poems['topic'].map(topic_dict)\no = OneHotEncoder()\no.fit(poems['topic'].values.reshape(-1,1))\n\npoems[[f'topic-{x}' for x in np.arange(144)]] = o.transform(poems['topic'].values.reshape(-1,1)).toarray()\n\nwords=[]\nfor poem in poems['poem']:\n    words.extend(poem)\n    \nwords = set(words)\n\ndictionary = defaultdict(default_factory=-1)\ndictionary.update(zip(words,np.arange(len(words))))\n    \npoems_embedded=[]\n\nfor i in np.arange(poems.shape[0]):\n    poems_embedded.append([dictionary[x] for x in poems.loc[i,'poem']])\n    \npoems['poems_embedded'] = poems_embedded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poems.drop(['poem','topic','poems_embedded'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class datamod(pl.LightningDataModule):\n    \n    def setup(self,stage):\n            \n        train_indices = np.random.choice(poems.index.tolist(), int(0.8 * poems.shape[0]))\n        remaining_indices = list(set(poems.index.tolist()).difference(set(train_indices)))\n        val_indices = np.random.choice(remaining_indices, int(0.1 * poems.shape[0]))\n        test_indices = list(set(remaining_indices).difference(set(val_indices)))\n        \n        ## VERY IMPORTANT : IF 0 IS NOT IN INDEX, TORCH.TENSOR() AND TORCH.FROM_NUMPY() DOESN'T WORK\n        \n        train_indices = np.append(train_indices, 0) if 0 not in train_indices else train_indices\n        val_indices = np.append(val_indices, 0) if 0 not in val_indices else val_indices\n        test_indices = np.append(test_indices, 0) if 0 not in test_indices else test_indices\n        \n        self.poems = poems\n        \n        self.X_train = self.poems.loc[train_indices, 'poems_embedded']\n        self.X_val = self.poems.loc[val_indices, 'poems_embedded']\n        self.X_test = self.poems.loc[test_indices, 'poems_embedded']\n        \n        \n        ## VERY IMPORTANT : CONVERTING A SERIES INTO A DF TO MAKE IT 2D AND HENCE INTO A NP ARRAY\n        \n        self.X_train = pd.DataFrame(self.X_train.tolist(), columns = np.arange(105)).values\n        self.X_val = pd.DataFrame(self.X_val.tolist(), columns = np.arange(105)).values\n        self.X_test = pd.DataFrame(self.X_test.tolist(), columns = np.arange(105)).values\n        \n        self.Y = self.poems.drop(['poem','topic','poems_embedded'], axis=1)\n        self.y_train = self.Y.loc[train_indices, :]\n        self.y_val = self.Y.loc[val_indices, :]\n        self.y_test = self.Y.loc[test_indices, :]\n        \n        self.train_dataset = torch.utils.data.TensorDataset(torch.Tensor(self.X_train), torch.Tensor(self.y_train.values))\n        self.val_dataset = torch.utils.data.TensorDataset(torch.Tensor(self.X_val), torch.Tensor(self.y_val.values))\n        self.test_dataset = torch.utils.data.TensorDataset(torch.Tensor(self.X_test), torch.Tensor(self.y_test.values))\n    \n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(self.train_dataset, batch_size=32)\n    \n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(self.val_dataset, batch_size=32)\n    \n    def test_dataloader(self):\n        return torch.utils.data.DataLoader(self.test_dataset, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class poem_classifier(pl.LightningModule):\n    \n    def __init__(self):\n        \n        super(poem_classifier, self).__init__()\n        \n        self.embedding = torch.nn.Embedding(num_embeddings=len(list(dictionary.keys())), embedding_dim=64)\n        self.lstm = torch.nn.LSTM(input_size = 64, hidden_size=32, num_layers = 1, batch_first=True)\n        self.fc1 = torch.nn.Linear(in_features=32, out_features=64)\n        self.fc2 = torch.nn.Linear(in_features=64, out_features=128)\n        self.fc3 = torch.nn.Linear(in_features=128, out_features=144)\n        \n    def forward(self, x):\n        \n        x = self.embedding(x)\n        x, _ = self.lstm(x)\n        x = x[:,-1]\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.nn.functional.softmax(self.fc3(x))\n        \n        return x\n    \n    def training_step(self, train_batch, batch_idx):\n        \n        X, y = train_batch\n        logits = self.forward(X.long())\n        loss = torch.nn.CrossEntropyLoss()\n        train_loss = loss(y, logits)\n        self.log('train_loss', train_loss, logger=True, prog_bar=True)\n        return train_loss\n    \n    def validation_step(self, val_batch, batch_idx):\n        \n        X, y = val_batch\n        logits = self.forward(X.long())\n        loss = torch.nn.CrossEntropyLoss()\n        val_loss = loss(y, logits)\n        self.log('val_loss',val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n        \n    def test_step(self, test_batch, batch_idx):\n        \n        self.eval()\n        X, y = test_batch\n        logits = self.forward(X.long())\n        loss = torch.nn.CrossEntropyLoss()\n        val_loss = loss(y, logits)\n        self.log('test_loss',val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n        \n    def predict_step(self,x):\n        \n        op = self.forward(x.long())\n        return np.argmax(op.detach().numpy())\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n        return optimizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.loggers import TensorBoardLogger\nlogger = TensorBoardLogger(\"lightning_logs\", name='poem-classification')\n%load_ext tensorboard\n!rm -rf ./logs/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = datamod()\nmodel = poem_classifier()\n\ntrainer = pl.Trainer(max_epochs=2, accelerator='gpu', logger=logger)\n\ntrainer.fit(model, data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %tensorboard --logdir ./lightning_logs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = data.test_dataloader()\ntrainer.test(model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict_step(torch.Tensor(poems.loc[2549, 'poems_embedded']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = model(next(iter(test_dataset))[0].long())\noutputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(outputs, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(next(iter(test_dataset))[1].numpy(),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}